{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "7632ae83",
      "metadata": {},
      "source": [
        "# Interactive Workbook: Time-Splitting Spectral Method for Scalar BEC (JAX)\n",
        "\n",
        "Welcome. This notebook is designed to build **deep intuition + implementation skill** for the split-step Fourier method applied to the Gross\u2013Pitaevskii equation (GPE).\n",
        "\n",
        "You will find:\n",
        "- **Conceptual checkpoints** (explain in your own words)\n",
        "- **Fill-in code blocks** (complete TODOs)\n",
        "- **Try-it-yourself experiments** (change parameters and reason about outcomes)\n",
        "- **Caveat/performance labs** (where methods fail or slow down)\n",
        "\n",
        "> Target equation (dimensionless):\n",
        "\n",
        "$$\n",
        "i\\partial_t\\psi = \\left[-\\frac{1}{2}\\nabla^2 + V(x,y) + g|\\psi|^2\\right]\\psi\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "14885872",
      "metadata": {},
      "source": [
        "## 0) Learning objectives\n",
        "\n",
        "By the end, you should be able to:\n",
        "1. Derive and implement Strang splitting for GPE.\n",
        "2. Explain why FFT diagonalizes the kinetic operator.\n",
        "3. Quantify accuracy via temporal/grid convergence tests.\n",
        "4. Diagnose stability/aliasing/normalization issues.\n",
        "5. Reason about performance tradeoffs (JIT, grid size, backend).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d523bb04",
      "metadata": {},
      "outputs": [],
      "source": "# If needed, run once:\n# !pip install jax jaxlib numpy matplotlib pandas scipy\n\nimport time\nimport numpy as np\nimport jax\nimport jax.numpy as jnp\nimport matplotlib.pyplot as plt\n\nfrom scalar_bec.solver import SolverConfig, run_simulation, make_grid, harmonic_potential, kspace_operators, normalize\nfrom scalar_bec.diagnostics import norm, energy, l2_error\n\nprint('JAX backend:', jax.default_backend())\nprint('Devices:', jax.devices())\n"
    },
    {
      "cell_type": "markdown",
      "id": "6780b589",
      "metadata": {},
      "source": [
        "## 1) Concept checkpoint: operator splitting\n",
        "\n",
        "We separate the Hamiltonian into:\n",
        "- $A = V + g|\\psi|^2$ (local in real space)\n",
        "- $B = -\\frac{1}{2}\\nabla^2$ (diagonal in Fourier space)\n",
        "\n",
        "Strang step:\n",
        "$$\n",
        "\\psi^{n+1} \\approx e^{-i\\frac{\\Delta t}{2}A} e^{-i\\Delta t B} e^{-i\\frac{\\Delta t}{2}A} \\psi^n\n",
        "$$\n",
        "\n",
        "### Questions\n",
        "1. Why is this second-order accurate in time?\n",
        "2. Why is the kinetic step cheap with FFT?\n",
        "3. What breaks if $\\Delta t$ is too large?\n",
        "\n",
        "_Write your answers in a markdown cell below._\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "13ca7cd0",
      "metadata": {},
      "source": [
        "### \u270d\ufe0f Your answers (insert markdown cell below this one)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Solutions (Concept checkpoint)\n\n1. **Why second-order?** Strang splitting is symmetric: half-step/full-step/half-step. The Baker\u2013Campbell\u2013Hausdorff expansion cancels odd-order commutator terms, leaving local truncation error $O(\\Delta t^3)$ and global error $O(\\Delta t^2)$.\n\n2. **Why FFT makes kinetic step cheap?** In Fourier space, $-\\nabla^2$ becomes multiplication by $k^2$. So the kinetic propagator is pointwise multiplication:\n$$\n\\hat{\\psi}(k) \\leftarrow e^{-i\\frac{\\Delta t}{2}k^2}\\hat{\\psi}(k),\n$$\nwith cost dominated by FFTs: $O(N\\log N)$.\n\n3. **What breaks for large $\\Delta t$?** Splitting error grows, phase errors accumulate, invariants drift more, and dynamics can become qualitatively wrong (even if numerically stable).\n"
    },
    {
      "cell_type": "markdown",
      "id": "aa075a6a",
      "metadata": {},
      "source": [
        "## 2) Fill-in code: build grid, potential, initial condition\n",
        "\n",
        "Complete the TODOs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed5d7a73",
      "metadata": {},
      "outputs": [],
      "source": "nx, ny = 128, 128\nlx, ly = 20.0, 20.0\ng = 100.0\ndt = 5e-4\nsteps = 200\n\ncfg = SolverConfig(nx=nx, ny=ny, lx=lx, ly=ly, g=g, dt=dt, steps=steps)\n\nx, y, X, Y, dx, dy = make_grid(cfg)\n\nV = harmonic_potential(X, Y, omega=1.0)\n\npsi0 = jnp.exp(-(X**2 + Y**2)/2.0).astype(jnp.complex64)\npsi0 = normalize(psi0, dx, dy)\n\nprint('dx, dy =', float(dx), float(dy))\nprint('Initial norm =', float(norm(psi0, dx, dy)))\n"
    },
    {
      "cell_type": "markdown",
      "id": "8f321d23",
      "metadata": {},
      "source": [
        "## 3) Fill-in code: one Strang step manually\n",
        "\n",
        "Implement a manual one-step update to see each operation explicitly.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "74ac820e",
      "metadata": {},
      "outputs": [],
      "source": "# Manual one-step implementation exercise\nkinetic_phase = kspace_operators(cfg)\nhalf = 0.5 * dt\n\nphase1 = jnp.exp(-1j * half * (V + g * jnp.abs(psi0)**2))\npsi_half = phase1 * psi0\n\npsi_k = jnp.fft.fft2(psi_half)\npsi_k = kinetic_phase * psi_k\npsi_full = jnp.fft.ifft2(psi_k)\n\nphase2 = jnp.exp(-1j * half * (V + g * jnp.abs(psi_full)**2))\npsi1 = phase2 * psi_full\n\nprint('Norm after one step =', float(norm(psi1, dx, dy)))\n"
    },
    {
      "cell_type": "markdown",
      "id": "901f1f41",
      "metadata": {},
      "source": [
        "### Concept check\n",
        "- Is norm exactly conserved numerically? Why/why not?\n",
        "- Which part introduces most floating-point error?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Solutions\n\n- **Norm conservation:** In exact arithmetic, each substep is unitary, so norm is conserved. In floating-point arithmetic, tiny drift appears from FFT roundoff and finite precision.\n- **Largest error source:** Typically FFT/ifft finite-precision roundoff plus repeated phase multiplications over many steps.\n"
    },
    {
      "cell_type": "markdown",
      "id": "cbe073eb",
      "metadata": {},
      "source": [
        "## 4) Run full simulation + diagnostics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5491db04",
      "metadata": {},
      "outputs": [],
      "source": "cfg = SolverConfig(nx=256, ny=256, lx=20.0, ly=20.0, g=100.0, dt=5e-4, steps=500)\n\nstart = time.perf_counter()\nout = run_simulation(cfg)\nout['psi'].block_until_ready()\nelapsed = time.perf_counter() - start\n\npsi = out['psi']\nV = out['V']\n\ndN = float(norm(psi, out['dx'], out['dy']))\nE = float(energy(psi, V, cfg.g, out['dx'], out['dy']))\n\nprint(f'backend={out[\"backend\"]}')\nprint(f'elapsed_s={elapsed:.4f}')\nprint(f'norm={dN:.8f}')\nprint(f'energy={E:.8f}')\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "edffab76",
      "metadata": {},
      "outputs": [],
      "source": "# Visualize density\nrho = np.array(jnp.abs(psi)**2)\n\nplt.figure(figsize=(5,4))\nplt.imshow(rho.T, origin='lower', cmap='magma', extent=[float(out['x'][0]), float(out['x'][-1]), float(out['y'][0]), float(out['y'][-1])])\nplt.colorbar(label='|psi|^2')\nplt.title('Final Density')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.tight_layout()\nplt.show()\n"
    },
    {
      "cell_type": "markdown",
      "id": "be7b09da",
      "metadata": {},
      "source": [
        "## 5) Try-it-yourself block: explore nonlinearity strength $g$\n",
        "\n",
        "Predict first, then run:\n",
        "1. What happens to density shape as $g$ increases?\n",
        "2. How does runtime change (if at all)?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c67e37c7",
      "metadata": {},
      "outputs": [],
      "source": "g_values = [0.0, 10.0, 100.0, 300.0]\nresults = []\n\nfor gtest in g_values:\n    cfg = SolverConfig(nx=256, ny=256, lx=20.0, ly=20.0, g=gtest, dt=5e-4, steps=300)\n    t0 = time.perf_counter()\n    out = run_simulation(cfg)\n    out['psi'].block_until_ready()\n    t1 = time.perf_counter()\n    results.append((gtest, t1-t0, float(norm(out['psi'], out['dx'], out['dy']))))\n\nfor row in results:\n    print(f'g={row[0]:6.1f}  elapsed={row[1]:.4f}s  norm={row[2]:.8f}')\n"
    },
    {
      "cell_type": "markdown",
      "id": "5494ab4a",
      "metadata": {},
      "source": [
        "## 6) Convergence study: error vs grid size\n",
        "\n",
        "We compare each grid to a high-resolution reference (subsampled).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dbe2b765",
      "metadata": {},
      "outputs": [],
      "source": "def resample_ref(ref, n):\n    stride = ref.shape[0] // n\n    return ref[::stride, ::stride]\n\nref_cfg = SolverConfig(nx=512, ny=512, steps=500, dt=2.5e-4)\nref_out = run_simulation(ref_cfg)\nref_psi = ref_out['psi']\n\ngrid_list = [64, 128, 256]\nerrs = []\n\nfor n in grid_list:\n    cfg = SolverConfig(nx=n, ny=n, steps=500, dt=2.5e-4)\n    out = run_simulation(cfg)\n    e = float(l2_error(out['psi'], resample_ref(ref_psi, n), out['dx'], out['dy']))\n    errs.append((n, e))\n    print(f'n={n:4d}, L2 error={e:.6e}')\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a12bced7",
      "metadata": {},
      "outputs": [],
      "source": "# Plot grid convergence\nnvals = np.array([e[0] for e in errs])\nerrvals = np.array([e[1] for e in errs])\n\nplt.figure(figsize=(5,4))\nplt.loglog(nvals, errvals, 'o-')\nplt.title('Grid Convergence')\nplt.xlabel('N')\nplt.ylabel('L2 error vs reference')\nplt.grid(True, which='both', alpha=0.3)\nplt.tight_layout()\nplt.show()\n"
    },
    {
      "cell_type": "markdown",
      "id": "42e0b0ad",
      "metadata": {},
      "source": [
        "### Concept question\n",
        "If the method is spectral in space, why might you *not* observe ideal exponential convergence in this test?\n",
        "\n",
        "Hints:\n",
        "- reference solution quality\n",
        "- subsampling strategy\n",
        "- finite precision\n",
        "- non-smooth features\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Solutions\n\nYou may miss ideal exponential convergence because:\n- the \"reference\" is not exact,\n- subsampling can introduce mismatch/aliasing,\n- finite precision limits asymptotic improvement,\n- nonlinear dynamics can generate sharper features that need higher resolution,\n- domain truncation/boundary effects contaminate error.\n"
    },
    {
      "cell_type": "markdown",
      "id": "dea7a1e5",
      "metadata": {},
      "source": [
        "## 7) Temporal convergence: error vs $\\Delta t$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b9462ff",
      "metadata": {},
      "outputs": [],
      "source": "ref_cfg = SolverConfig(nx=256, ny=256, steps=2400, dt=1.25e-4)\nref = run_simulation(ref_cfg)['psi']\n\ntests = [(1e-3, 300), (5e-4, 600), (2.5e-4, 1200)]\nrows = []\nfor dt_test, steps_test in tests:\n    cfg = SolverConfig(nx=256, ny=256, dt=dt_test, steps=steps_test)\n    out = run_simulation(cfg)\n    rmse = float(jnp.sqrt(jnp.mean(jnp.abs(out['psi'] - ref)**2)))\n    rows.append((dt_test, rmse))\n    print(f'dt={dt_test:.2e}, rmse={rmse:.6e}')\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e383a51",
      "metadata": {},
      "outputs": [],
      "source": "dts = np.array([r[0] for r in rows])\nerm = np.array([r[1] for r in rows])\n\nplt.figure(figsize=(5,4))\nplt.loglog(dts, erm, 'o-')\nplt.gca().invert_xaxis()\nplt.title('Temporal Convergence')\nplt.xlabel('dt')\nplt.ylabel('RMSE vs reference')\nplt.grid(True, which='both', alpha=0.3)\nplt.tight_layout()\nplt.show()\n"
    },
    {
      "cell_type": "markdown",
      "id": "8d3edb77",
      "metadata": {},
      "source": [
        "### Fill-in: estimate convergence order\n",
        "For two runs $(\\Delta t_1, e_1), (\\Delta t_2, e_2)$, the empirical order is:\n",
        "$$\n",
        "p \\approx \\frac{\\log(e_1/e_2)}{\\log(\\Delta t_1/\\Delta t_2)}\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "(dt1, e1), (dt2, e2) = rows[0], rows[-1]\np = np.log(e1/e2)/np.log(dt1/dt2)\nprint('Empirical temporal order p =', p)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8) Performance lab: warmup, JIT, and measurement pitfalls\n",
        "\n",
        "Important caveat: first JAX call includes compile overhead.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "cfg = SolverConfig(nx=256, ny=256, steps=300)\n\n# First run (compile + execute)\nt0 = time.perf_counter()\nout = run_simulation(cfg)\nout['psi'].block_until_ready()\nt1 = time.perf_counter()\n\n# Second run (mostly execute)\nt2 = time.perf_counter()\nout2 = run_simulation(cfg)\nout2['psi'].block_until_ready()\nt3 = time.perf_counter()\n\nprint(f'first_run_s  = {t1-t0:.4f}')\nprint(f'second_run_s = {t3-t2:.4f}')\nprint('speedup from removing compile overhead:', (t1-t0)/(t3-t2))\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Try-it-yourself\n",
        "1. Change grid to 384 and 512. How does scaling behave?\n",
        "2. Repeat 3 times and report median runtime.\n",
        "3. On GPU, compare FFT-heavy workload vs CPU.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9) Optional C++ bridge (discussion + exercise)\n",
        "\n",
        "You have `scalar_bec_cpp` available. In this project, JAX JIT/XLA often beats naive Python loops and can fuse ops efficiently.\n",
        "\n",
        "### Conceptual question\n",
        "When would a custom C++ (or CUDA) kernel still help?\n",
        "- unsupported operation patterns in XLA\n",
        "- custom memory layout control\n",
        "- specialized fused kernels not produced by compiler\n",
        "\n",
        "### Exercise\n",
        "Try replacing nonlinear phase update with a host callback to C++ and benchmark. Does it help or hurt? Why?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Suggested answer\n\nA custom C++/CUDA kernel can help when:\n- XLA cannot fuse the needed sequence efficiently,\n- memory traffic dominates and custom fusion reduces reads/writes,\n- you need specialized numerics/layouts not expressible in JAX primitives,\n- integrating with existing HPC kernels/libraries.\n\nIt can hurt when host-device transfers or callback boundaries break JIT fusion.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10) Caveats and failure modes checklist\n",
        "\n",
        "- **Aliasing:** nonlinear term can populate high-k modes.\n",
        "- **Boundary artifacts:** FFT implies periodic boundaries.\n",
        "- **Time-step too large:** phase errors accumulate; wrong dynamics.\n",
        "- **Energy drift:** monitor when changing precision/backend.\n",
        "- **Reference error contamination:** convergence studies depend on trustworthy reference.\n",
        "- **Precision choice:** complex64 is faster but less accurate than complex128.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11) Capstone mini-project\n",
        "\n",
        "Implement and test one of:\n",
        "1. **Imaginary-time propagation** for ground state.\n",
        "2. **Rotating frame + vortex initial condition.**\n",
        "3. **Dealiasing filter** and show effect on stability/error.\n",
        "\n",
        "For your report include:\n",
        "- equation modifications,\n",
        "- implementation details,\n",
        "- runtime + accuracy comparison,\n",
        "- at least one plot and one caveat.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Bonus prompt (for your own notes)\n",
        "\n",
        "In 5\u201310 bullets, explain *why* the split-step Fourier method is a strong default for nonlinear Schr\u00f6dinger/GPE problems, and where you\u2019d avoid it.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}